{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Laboratório 3: Classificando textos com a base de dados IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Bem vindo ao laboratório 3 do Mini curso de TensorFlow, neste tutorial vamos aprender a classificar avaliacoes de filmes utilizando o TensorFlow 2 e o keras.\n",
    "\n",
    "<img src=\"imgs/tf_logo_social.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "O objetivo deste laboratorio será classificar avaliações de filmes em **positivas** ou **negativas** utilizando uma rede neural que construiremos no decorrer deste tutorial e um bastante aplicado tipo de problema de aprendizado de máquina conhecido como classificação binária. Para isso, utilizaremos a base de dados do site IMDB (Internet Movie Database) que possui avaliaçòes de mais de 50000 filmes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importando as bibliotecas necessárias para o projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para realizarmos a construção da nossa rede neural com o Keras, precisamos importar algumas bibliotecas essenciais para a construção da nossa rede e algumas outras bibliotecas que nos ajudarão a ter uma melhor visualização e manipulação dos dados. O código abaixo realiza as importações necessárias, não se preocupe pois as respectivas bibliotecas já se encontram devidamente instaladas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Importando o TensorFlow e tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Bibliotecas auxiliares\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verificamos se a importação foi realizada\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Caso tenha sido impresso a versão do Tensorflow instalada, a importação foi feita com sucesso e o próximo passo será conhecer melhor o nosso dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conhecendo o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Utilizaremos como dataset a base de dados **IMBD** que está disponível junto ao TensorFlow, esta base de dados se encontra pré-processada de forma que as avaliações (ou sequências de palavras) foram convertidas em sequências de inteiros, onde cada inteiro representa uma palavra específica no dicionário. Utilizaremos o método *load_data()* para carregar os dados e separá-los em dois grupos: um grupo para testes com 25.000 avaliações e um grupo para treinamento com também 25.000 avaliações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_imdb = keras.datasets.imdb\n",
    "\n",
    "(dados_treinamento, dados_treinamento_labels), (dados_teste, dados_teste_labels) = dataset_imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe que definimos um parâmetro no método *load_data()*  conhecido como *num_words*, este parâmetro em nosso exemplo indica que queremos obter as 10000 palavras mais frequentes presentes no conjunto de treinamento. As palavras menos frequêntes não possuem muita relevância para o treinamento e seu uso pode aumentar a complexidade da rede e prejudicar o treinamento, por este fato estas palavras serão descartadas. Vamos primeiro imprimir tamanho total de avaliações presente nos conjuntos de teste e treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Tamanho do conjunto de Treinamento: {}\".format(len(dados_treinamento)))\n",
    "\n",
    "print(\"Tamanho do conjunto de Teste: {}\".format(len(dados_teste)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Como podemos observar, temos cerca de 25.000 avaliações para cada conjunto (teste e treinamento). Os *labels* correspondem a um valor inteiro com valor de 0 ou 1, onde 0 é uma avaliação negativa e 1 respresenta uma avaliação positiva. Para conhecermos mais sobre o nosso dataset, vamos visualizar a primeira avaliação presente no conjunto de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(dados_treinamento[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A saída acima nos mostra uma sequência de números presentes em uma lista, cada número desse representa uma palavra dentro do vocabulário conhecido em nossa base de dados. O tamanho total do vocabulário existente em nosso dataset será cerca de 80.000 palavras, lembrando que durante o treinamento utilizaremos no conjunto de treinamento e teste apenas as palavras de maior utilização e consequêntemente maior relevância para o treinamento. Mas simplesmente dizer que esta sequência de números corresponde a uma avaliação de algum filme não é aceitável, precisamos também construir um método para traduzir esses valores em palavras novamente e assim podermos visualizar melhor os dados em seu estado original. Primeiro precisamos carregar o vocabulário para depois podermos começar a trazuir as avaliações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "palavras_vocabulario = dataset_imdb.get_word_index()\n",
    "\n",
    "print(\"Número total de palavras: {} | Tipo: {}\".format(len(palavras_vocabulario),type(palavras_vocabulario)))\n",
    "# print(palavras_vocabulario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Em seguida precisamos formartar o nosso vocabulário, adicionando mais alguns valores que representam aspectos do texto como ponto de inicio e caracter desconhecido para o vocabulário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "palavras_vocabulario = {k:(v+3) for k,v in palavras_vocabulario.items()}\n",
    "palavras_vocabulario[\"<PAD>\"] = 0 # preenchimento (mais a frente fará sentido)\n",
    "palavras_vocabulario[\"<START>\"] = 1 # inicio do texto\n",
    "palavras_vocabulario[\"<UNK>\"] = 2  # caractere desconhecido (unknown)\n",
    "palavras_vocabulario[\"<UNUSED>\"] = 3 # caractere não utilizado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finalizamos a formatação invertendo os valores das chaves com seus respectivos valores inteiros, basicamente o que antes era *{\"TensorFlow\": 200}* se transforma em *{200 : \"TensorFlow\"}*. Esta manipulação nos permitirá manipular melhor o vocabulário, \"revertendo\" as posições de valores: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "palavras_vocabulario_reverso = dict([(value, key) for (key, value) in palavras_vocabulario.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "É válido destacar que a variável *palavras_vocabulario_reverso* somente terá utilizada na tradução dos valores inteiros para palavras. Finalizamos a parte de tradução, criando um metodo chamado *decodifica_avaliacao()* que ficará responsável por traduzir os valores para palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def decodifica_avaliacao(texto):\n",
    "    return ' '.join([palavras_vocabulario_reverso.get(i, '?') for i in texto])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finalmente podemos verificar se o nosso método *decodifica_avaliacao()* é capaz de traduzir perfeitamente as avaliações e assim nos permitir visualizá-las para conhecer melhor o dataset. Vamos imprimir inicialmente a primeira avaliação contida no conjunto de treinamento e em seguida exibiremos a sua respectiva tradução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Avaliação original: {}\".format(dados_treinamento[0]))\n",
    "\n",
    "print(\"\\nAvaliação traduzida: {}\".format(decodifica_avaliacao(dados_treinamento[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparando os dados para o treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Um ponto importante que devemos observar está no fato das avaliações presentes no dataset possuírem tamanhos diferentes, tal característica não é desejável para a rede que iremos montar. Para montarmos uma rede neural, precisamos conhecer as dimensões que os dados devem ter e estas não podem variar, pois logo na camada de entrada teremos um número de elementos esperados que irão impactar diretamente no aprendizado. Por tanto, é preciso padronizar os dados de entrada de forma a todas as amostras possuírem um tamanho único máximo sem afetar o treinamento. Abaixo podemos conferir melhor o tamanho das duas primeiras avaliações presentes no conjunto de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Tamanho avaliação 1: {} | Tamanho avaliação 2: {}\".format(len(dados_treinamento[0]), len(dados_treinamento[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Partindo desta análise, concluímos que as avaliações devem ser convertidas em tensores de tamanho único antes de alimentar a rede neural. Para isso podemos seguir duas abordagens bem interessantes:\n",
    "\n",
    "* Converter as avaliações em vetores de 0s e 1s indicando a ocorrência da palavra, o tamanho total deste vetor seria o total de palavras no vocabulário e cada uma corresponderia a um respectivo índice. De forma que teríamos um vetor preenchido com o valor 1 apenas nas posições em que as palavras do vocabulário estejam presentes na respectiva avaliação. Tal abordagem é intensa em relação a memória, logo requer uma matriz de tamanho numero_palavras * numero_avaliacoes.\n",
    "\n",
    "* Definir um tamanho máximo de caracteres para cada avaliação e preencher o array para que todos tenho o mesmo comprimento. O custo de memória nesse caso se torna menor, resumindo apenas a um tensor inteiro de tamanho tamanho_maximo * numero_avaliacoes.\n",
    "\n",
    "Para diminuir o consumo de memória e otimizar a velocidade de treinamento utilizaremos a segunda abordagem. Para isso, precisaremos utilizar o método *keras.preprocessing.sequence.pad_sequences()* para podermos converter os conjuntos de treinamento e teste para um tamanho único que em nosso caso será de 256 palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dados_treinamento = keras.preprocessing.sequence.pad_sequences(dados_treinamento,\n",
    "                                                        value=palavras_vocabulario[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "dados_teste = keras.preprocessing.sequence.pad_sequences(dados_teste,\n",
    "                                                       value=palavras_vocabulario[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Definindo a arquitetura da rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Antes mesmo de começarmos a construir a nossa rede neural, precisamos primeiramente definir a sua arquitetura que irá variar conforma a precisão que se busca e a característica do estudo de caso em que estamos trabalhando. Tudo que temos até este momento são os nossos dados de entrada organizados em vetores de tamanho único, mas para termos um bom desempenho e tornar nossa rede computacionalmente viável precisamos definir algumas camadas para pré-processamento dos dados que tornarão nossa rede mais eficiente. Nossa rede neural então irá possuir 4 camadas:\n",
    "\n",
    "* Uma camada de entrada que será capaz de permitir que a nossa rede também interprete o contexto de cada palavra no texto, por meio de uma técnica conhecida como *embedding*.\n",
    "* Uma camada oculta  que irá permitir o modelo lidar com entradas de tamanhos diferentes da maneira mais simples possível.\n",
    "* Uma camada oculta para o aprendizado e reconheimento de padrões.\n",
    "* Uma camada de saída para  que irá nos mostrar como saída se a avaliação é **positiva** ou **negativa**.\n",
    "\n",
    "A camada inicial da nossa rede será uma *Embedding layer*, uma camada muito utilizada no processamento de linguagem natural e que nos permite analizar as relações de proximidade de cada palavra em um dado contexto. Essa camada gera uma vetores *embedding* para cada palavra de nosso vocabulário que são atualizados durante o treinamento da rede, permitindo que nossa rede visualize as semelhanças entre cada palavra e identificar os relacionamentos entre elas. Para explicar melhor como é esta transformação vamos supor a seguinte frase de exemplo:\n",
    "\n",
    "> Este minicurso está muito bom\n",
    "\n",
    "Para a frase acima podemos agrupar cada palavra com um respectivo índice e ordená-los na forma de um vetor:\n",
    "\n",
    "> [1,2,3,4,5]\n",
    "\n",
    "Agora para realizarmos a incorporação dos dados (Embedding), precisamos definir o número de fatores latentes para cada palavra de nosso vocabulário, este valor corresponde ao número de características implícitas que cada palavra poderá ter, tais como sinônimos ou forma verbal conforme o contexto da frase. Em nosso exemplo com fins explicativos, definimos arbitrariaamente cinco fatores, a matriz de vetores *embedding* gerada seria a seguinte:\n",
    "\n",
    "<img src=\"imgs/tabela_exemplo.png\" width=\"500\">\n",
    "\n",
    "\n",
    "Ao invés de termos apenas como entrada para nossa rede um vetor compostos por **0**s e **1**s, agora conseguimos obter um vetor único composto por valores mais complexos e de dimensão fixa que nos permite obter muito mais dados que anteriormente seria possível. Voltando a construção de nossa rede, na camada *Embedding* definimos arbitrariamente o número de 16 fatores latentes, logo para cada palavra em nosso vocabulário teremos um vetor de tamanho 16. A imagem a seguir ilustra melhor esta camada:\n",
    "\n",
    "<img src=\"imgs/camada_de_entrada.png\" width=\"500\">\n",
    "\n",
    "A nossa camada de entrada gera como saída inúmeros vetores com n dimensões, o que torna os dados consideravelmente custosos em termos de processamento. Para simplificar ainda mais nosso exemplo, utilizaremos a *GlobalAveragePooling1D* como nossa primeira camada oculta, esta camada realiza apenas o redimensionamento dos dados para apenas uma única dimensão ao tirar uma média dos valores existentes. \n",
    "\n",
    "A próxima camada oculta, será a responsável pelo aprendizado da nossa rede, esta camada é muito importante e deve ser construída com cautela pois pode prejudicar a precisão ou até mesmo elevar o tempo de treinamento desnecessariamente. Vamos definir um valor de 16 neurônios para esta camada, e por ser uma camada responsável pelo aprendizado, será necessário definir uma função de ativação para esta camada que vai permitir saber se a informação que o neurônio está recebendo é relevante para a informação fornecida ou deve ser ignorada. A função de ativação escolhida para esta camada será a ReLU (linear rectificadora) que é uma função amplamente utilizada ao projetar redes neurais atualmente e capaz de trabalhar com problemas não lineares. Voltando a topologia da nossa rede neural, a camada oculta possuirá a seguinte estrutura:\n",
    "\n",
    "<img src=\"imgs/camada_oculta_2.png\" width=\"200\">\n",
    "\n",
    "Em seguida precisamos definir a topologia da nossa ultima camada, nela é interessante termos como resultado se a avaliação é **positiva** ou **negativa** podendo a saída ser prepresentada pelo valor de **1** ou **0**. Para obtermos este tipo de representação, utilizaremos uma função de ativação conhecida como sigmoid. A função de ativação sigmóide muito utilizada em problemas de classificação com uma única classe que nos gera como resultado valores de **1** ou **0**. A topologia da ultima camada da nossa rede irá possuir apenas 1 neurônio onde a saída **1** corresponde a uma avaliação positiva e a saída **0** a uma avaliação negativa, conforme podemos notar a seguir:\n",
    "\n",
    "<img src=\"imgs/camada_saida.png\" width=\"200\">\n",
    "\n",
    "Outro fato importante a se destacar é que a nossa rede neural possui algumas camadas completamente conectadas, o que significa que todos os neurônios de uma camada se conectam a todos os neurônios da camada seguinte. Por tanto a topologia final da rede será a seguinte:\n",
    "\n",
    "\n",
    "<img src=\"imgs/topologia_final.png\" width=\"800\">\n",
    "\n",
    "\n",
    "O passo seguinte será a construção do nosso modelo da rede neural que acabamos de definir, nesta etapa iremos configurar as camadas do modelo e logo em seguida compilar o modelo para podermos prosseguir com o treinamento da rede.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Construindo o modelo da nossa rede neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Uma vez definido a arquitetura da nossa rede, precisamos agora construir o modelo no TensorFlow que irá representar a arquitetura definida. Para isso utilizaremos o método keras.Sequential() onde iremos passar como parâmetro uma lista que irá conter cada camada da nossa rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tamanho_vocabulario = 10000\n",
    "\n",
    "modelo = keras.Sequential()\n",
    "modelo.add(keras.layers.Embedding(tamanho_vocabulario, 16))\n",
    "modelo.add(keras.layers.GlobalAveragePooling1D())\n",
    "modelo.add(keras.layers.Dense(16, activation='relu'))\n",
    "modelo.add(keras.layers.Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe que cada camada foi definida no modelo seguindo a arquitetura proposta:\n",
    "\n",
    "* **Camada de entrada** - A primeira camada foi justamente a camada *Embedding layer* (*keras.layers.Embedding*), uma camada muito utilizada no processamento de linguagem natural e que nos permite analizar as relações de proximidade de cada palavra em um dado contexto. Seus parâmetros foram o número de palavras existentes no vocabulário, que no nosso estudo de caso é cerca de 10000 palavras, e o número de fatores latentes.\n",
    "* **Primeira camada oculta** - A primeira camada oculta foi uma camada *GlobalAveragePooling1D*, esta camada realiza apenas o redimensionamento dos dados para apenas uma única dimensão ao tirar uma média dos valores existentes.\n",
    "* **Segunda camada oculta** - A segunda camada oculta foi definida pelo metodo *keras.layers.Dense()*, uma camada completamente conectada responsável pelo aprendizado da nossa rede. Esta camada recebe como parâmetros iniciais o número de neurônios desejados para esta camada e o tipo de função de ativação que esta camada deve trabalhar que no nosso caso foi a ReLU.\n",
    "* **Camada de saída** - A ultima camada foi definida pelo método *keras.layers.Dense()*, recebendo como parâmetros o número de neurônios que em nosso caso será de apenas 1, devido ao fato de termos apenas uma classe, e o tipo de função de ativação que desejamos que esta camada tenha. Onde este neurônio terá como saída valores de **1** ou **0** correspondente a classificação da avaliação inserida na rede.\n",
    "\n",
    "Alguns parâmetros extras devem ser configurados para que o nosso modelo possa prosseguir para a etapa de treinamento pois envolvem aspectos como medição de precisão de treinamento e atualização dos pesos e bias, esses parâmetros são inseridos no passo de compilação:\n",
    "\n",
    "* **Optimizer** - Este parâmetro define como o modelo deve se atualizar com base no dado que ele vê e sua função loss;\n",
    "* **Loss** - Este parâmetro mede o quão preciso o modelo é durante o treinamento;\n",
    "* **Metrics** - Este parâmetro serve para monitorar os passos de treinamento e teste, onde no nosso caso irá monitorar a precisão que o treinamento da nossa rede está obtendo.\n",
    "\n",
    "O código abaixo corresponde a compilação do modelo e a definição destes parâmetros complementares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Treinando a rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para o treinamento da nossa rede, vamos utilizar o método *.fit()* que nada mais é do que um método do TensorFlow responsável pelo treinamento dos modelos de redes neurais para um número específico de épocas. Uma época nada mais é do que a apresentação de todos os elementos do conjunto de treinamento no processo de aprendizado, ou seja, basicamente representará o número de vezes que determinado dado irá passar pela nossa rede durante o treinamento. Ao chamarmos este método, além do número de épocas, precisamos repassar as imagens de treinamento, seus respectivos labels e o conjunto de validação do treinamento (que no nosso caso será composto por parte dos dados presentes no conjunto de testes), conforme observa-se abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "historico = modelo.fit(dados_treinamento,\n",
    "                    dados_treinamento_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(dados_teste[0:10000], dados_teste_labels[0:10000]),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Conforme o modelo treina, valores de perda (loss) e precisão (acurácia) são exibidos, em nosso exemplo obtivemos uma média de 85% de precisão em nosso treinamento. Ter mais de uma época de treinamento é essencial para a precisão da nossa rede, uma vez que os dados são sequenciados aleatóriamente para o treinamento, uma unica sequência de imagens pode afetar a precisão da nossa rede durante o processo de aprendizado. Outro ponto a destacar foi o parâmetro *batch_size* que divide o conjunto de treinamento em  mini-batches com 512 exemplos. Em nosso exemplo definimos apenas 20 épocas para o treinamento, este valor é arbitrário e impacta diretamente no tempo de treinamento da sua rede e consequêntemente na precisão da mesma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Avaliando a precisão da rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para avaliarmos a precisão da rede neural que construimos e treinamos, vamos utilizar o conjunto de treinamento como entrada do metodo evaluate() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "valor_perda, valor_precisao = modelo.evaluate(dados_teste[10000:], dados_teste_labels[10000:])\n",
    "print('\\nPrecisão na classificação das amostras de teste:', valor_precisao) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Observe que a precisão obtida ao utilizar o conjunto de teste é um pouco menor do que a precisão exibida na etapa de treinamento. Essa diferença representa um *overfitting*. O *overfitting* ocorre quando o modelo de aprendizado de máquina funciona bem para os dados existentes, mas não tem um bom resultado para novas situações. Vamos confirmar a precisão da nossa rede para o conjunto das amostras de treinamento e assim visualizar melhor a diferença:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "valor_perda_treinamento, valor_precisao_treinamento = modelo.evaluate(dados_treinamento, dados_treinamento_labels)\n",
    "print('\\nPrecisão na classificação das amostras de treinamento:', valor_precisao_treinamento) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Como podemos observar, para o conjunto de treinamento a precisão foi na casa dos 90% enquanto a precisão para o conjunto de teste foi de 85% que é a precisão real da rede que desenvolvemos, o que nos mostra a importância de se utilizar para teste sempre um conjunto de dados que esteja fora do conjunto de treinamento para evitar situações como *overfitting* e assim garantir que os resultados do processo de treinamento e construção da rede não sejam afetados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Analizando a evolução do treinamento ao longo das épocas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Um recurso interessante que podemos utilizar é o histórico de tudo o que ocorreu durante o treinamento de nossa rede neural. A variável *historico*, que definimos para receber o retorno do método *fit()* durante o treinamento, nos permite obter todo o hitórico a respeito da evolução do treinamento ao passar das épocas. Esses dados ficam agrupados em um dicionário e nos permite obter dados de precisão e perda: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "historico_dict = historico.history\n",
    "\n",
    "print(\"Chaves contidas no dicionario: {}\".format(historico_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Com estes dados, conseguimos facilmente organizar estes dados e exibí-los na forma de dois gráficos para então compararmos a evolução da precisão e perda durante o treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "precisao = historico_dict['accuracy']\n",
    "valor_precisao = historico_dict['val_accuracy']\n",
    "perda = historico_dict['loss']\n",
    "valor_perda = historico_dict['val_loss']\n",
    "\n",
    "epocas = range(1, len(precisao) + 1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(30,10))\n",
    "# Montando o gráfico do valor de loss (perda)\n",
    "plt.subplot(1,2,1)\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.title('Evolução do valor de perda')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.plot(epocas, perda, 'ro', label='Dataset de treinamento')\n",
    "plt.plot(epocas, valor_perda, 'r', label='Dataset de validação')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Montando o gráfico do valor de precisão\n",
    "plt.subplot(1,2,2)\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.title('Evolução do valor de precisão')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Precisão')\n",
    "plt.plot(epocas, precisao, 'bo', label='Dataset de treinamento')\n",
    "plt.plot(epocas, valor_precisao, 'b', label='Dataset de validação')\n",
    "plt.legend()\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Os dois gráficos acima nos permitem obter conclusões interessantes. A primeira é que conforme o número de épocas aumenta menor fica o valor de perda e consequentemente melhor fica nossa rede. Outra conclusão interessante é que conforme o número de épocas aumenta, a precião também aumenta até certo ponto onde se torna estável e praticamente não se altera, nos indicando que para obter uma precisão maior será necessário melhorar a nossa rede. Também é possível visualizar o *overfitting* ocorrendo, observe que no gráfico correspondente a precisão da rede os valores utilizando o dataset de treinamento são sempre superiores aos valores utilizando o dataset de validação, isso indica que nosso modelo esta se saindo melhor com dados que ele já viu e pior com dados novos que não participarão do treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}